%
% File naaclhlt2013.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2013}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{CIS 530 Final Project: Automatic Summarization Systems}

\author{Jingyi Wu \\
	    {\tt wujingyi@seas.upenn.edu}
	  \And
    Emily Boggs\\
  {\tt emboggs@seas.upenn.edu}}

\begin{document}
\maketitle
\begin{abstract}
For this project, we have implemented three basic summarizers: a TF*IDF summarizer, a LexRank summarizer, and a KL-divergence summarizer. In addition, we have designed a new summarizer that ranks sentences using a feature-based classifier.
\end{abstract}

\section{General Parameters}
Some system parameters are consistent for all of the summarizers. 
\begin{itemize}
\item Each summarizer takes two parameters: an input collection with groups of documents to summarize, and the output folder in which to save the summaries.
\item The summarizer output is named by prefixing the name of the current subdirectory with ''sum\_''. For instance, the input collection subdirectory ''dev\_00'' would produce the summary ''sum\_dev\_00.txt''.
\item The sentences of the input collections are always lowercased. 
\item Only sentences that are between 9 and 45 words long are considered for inclusion in the summary.
\item To reduce redundancy, sentences that exceed a certain threshold of similarity with any sentence already in the summary are not added. The exact value of the threshold differs between implementations.
\item For TF*IDF calculations, IDF was computed from the New York Times corpus. Because this corpus does not contain every word in the test inputs, IDF calculations used add-one smoothing.


\section{Basic Systems}

\subsection{TF*IDF System}
The TF*IDF Summarizer ranks sentences by calculating the average TF*IDF score for the words in each sentence. In this implementation, stopwords are included in the calculation, because the proportion of stopwords to content-ful words is an important aspect of the TF*IDF ranking.
The sentence score is calculated over all word types, instead of all tokens. In tests with the development data, using types instead of tokens was found to result in a small increase in ROUGE-2 recall.
In reducing redundancy, the best similarity threshold was found to be 0.8, which resulted in the highest ROUGE-2 recall for the development data. 

\subsection{LexRank System}
DESCRIBE LEXRANK HERE

\subsection{Kl Divergence System}
The KL Summarizer selects sentences to add to the summary by greedily choosing the sentence that will minimize KL divergence between the summary and the input. For this implementation, stopwords are ignored in all calculations. Because unigram distribution of stopwords is less likely to be substantially different between the summary and the input, removing them would highlight the meaningful words in the distribution. No smoothing was performed in calculating the distributions; since KL is a sum of probability calculations, a zero term does not significantly affect the output. 

\subsection{Performance on Development Set}
\vskip 0.5em
\begin{tabular}{|c|c|c|c|}
  \hline
  System & TF*IDF & LexRank & KL Divergence \\ \hline
  Rouge-2 Recall & *.** & *.** & *.** \\
  \hline
\end{tabular}

\section{Your Summarization System}

\subsection{System Design}

This part shows general idea of your system. You may use flowchart, graphics or pseudo-code to describe your algorithm.

\subsection{Resources \& Tools Used}

What resources or tools you have used and how they are included in your implementations.
\vskip 1em
\noindent Example: Wordnet, Stanford NER, MPQA, TopicS.
\vskip 1em
\noindent I use Stanford-Parser in order to help \dots

\subsection{Performance}

\section{Discussion and Analysis}

\begin{thebibliography}{}

\bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
Alfred~V. Aho and Jeffrey~D. Ullman.
\newblock 1972.
\newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
\newblock Prentice-{Hall}, Englewood Cliffs, NJ.

\bibitem[\protect\citename{{American Psychological Association}}1983]{APA:83}
{American Psychological Association}.
\newblock 1983.
\newblock {\em Publications Manual}.
\newblock American Psychological Association, Washington, DC.

\bibitem[\protect\citename{{Association for Computing Machinery}}1983]{ACM:83}
{Association for Computing Machinery}.
\newblock 1983.
\newblock {\em Computing Reviews}, 24(11):503--512.

\bibitem[\protect\citename{Chandra \bgroup et al.\egroup }1981]{Chandra:81}
Ashok~K. Chandra, Dexter~C. Kozen, and Larry~J. Stockmeyer.
\newblock 1981.
\newblock Alternation.
\newblock {\em Journal of the Association for Computing Machinery},
  28(1):114--133.

\bibitem[\protect\citename{Gusfield}1997]{Gusfield:97}
Dan Gusfield.
\newblock 1997.
\newblock {\em Algorithms on Strings, Trees and Sequences}.
\newblock Cambridge University Press, Cambridge, UK.

\end{thebibliography}

\end{document}
