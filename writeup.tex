%
% File naaclhlt2013.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2013}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{CIS 530 Final Project: Automatic Summarization Systems}

\author{Jingyi Wu \\
	    {\tt wujingyi@seas.upenn.edu}
	  \And
    Emily Boggs\\
  {\tt emboggs@seas.upenn.edu}}

\begin{document}
\maketitle
\begin{abstract}
For this project, we have implemented three basic summarizers: a TF*IDF summarizer, a LexRank summarizer, and a KL-divergence summarizer. In addition, we have designed a new summarizer that ranks sentences using a feature-based classifier.
\end{abstract}

\section{General Parameters}
Some system parameters are consistent for all of the summarizers. 
\begin{itemize}
\item Each summarizer takes two parameters: an input collection with groups of documents to summarize, and the output folder in which to save the summaries.
\item The summarizer output is named by prefixing the name of the current subdirectory with ''sum\_''. For instance, the input collection subdirectory ''dev\_00'' would produce the summary ''sum\_dev\_00.txt''.
\item The sentences of the input collections are always lowercased. 
\item Only sentences that are between 9 and 45 words long are considered for inclusion in the summary.
\item To reduce redundancy, sentences that exceed a certain threshold of similarity with any sentence already in the summary are not added. The exact value of the threshold differs between implementations.
\item For TF*IDF calculations, IDF was computed from the New York Times corpus. Because this corpus does not contain every word in the test inputs, IDF calculations used add-one smoothing.
\end{itemize}

\section{Basic Systems}

\subsection{TF*IDF System}
The TF*IDF Summarizer ranks sentences by calculating the average TF*IDF score for the words in each sentence. In this implementation, stopwords are included in the calculation, because the proportion of stopwords to content-ful words is an important aspect of the TF*IDF ranking.
The sentence score is calculated over all word types, instead of all tokens. In tests with the development data, using types instead of tokens was found to result in a small increase in ROUGE-2 recall.
In reducing redundancy, the best similarity threshold was found to be 0.8, which resulted in the highest ROUGE-2 recall for the development data. 

\subsection{LexRank System}
DESCRIBE LEXRANK HERE

\subsection{Kl Divergence System}
The KL Summarizer selects sentences to add to the summary by greedily choosing the sentence that will minimize KL divergence between the summary and the input. For this implementation, stopwords are ignored in all calculations. Because unigram distribution of stopwords is less likely to be substantially different between the summary and the input, removing them would highlight the meaningful words in the distribution. No smoothing was performed in calculating the distributions; since KL is a sum of probability calculations, a zero term does not significantly affect the output. 
The threshold for removing redundant sentences is set to 1, which means that sentences are not ignored on the basis of similarity to sentences already in the summary. This value was chosen based on tests with development data; one possible explanation for the result is that the KL computation takes care of redundancy issues itself, causing any further effort to be counterproductive.

\subsection{Performance on Development Set}
\vskip 0.5em
\begin{tabular}{|c|c|c|c|}
  \hline
  System & TF*IDF & LexRank & KL  \\ \hline
  Rouge-2 Recall & *.** & *.** & *.**** \\
  \hline
\end{tabular}
%\begin{tabular}{|c|c|}
%  \hline
%  System & Rouge-2 Recall \\ \hline
%  TF*IDF & *.** \\ \hline
%  LexRank & *.** \\ \hline
%  KL Divergence & *.** \\ \hline
%\end{tabular}


\section{Your Summarization System}

\subsection{System Design}

This part shows general idea of your system. You may use flowchart, graphics or pseudo-code to describe your algorithm.

\subsection{Resources \& Tools Used}
\vskip 1em
\noindent SRILM is used to create a language model of the dev\_model summaries. Sentences are ranked by their perplxity in the language model, to automatically generate labels for training the classifier. Sentences with lower perplexity are assumed to be more likely to be in the summary.
\vskip 1em
\noindent WordNet is used in one of the classifier feature groups. The specificity of each word in a sentence is computed as the distance between the word and the root hypernym in WordNet. Each word is designated as specific, general, or medium according to pre-determined thresholds. The counts of each of these word types, as well as the average specificity of the words in the sentence, are used as feature values. The motivation for this feature is the idea that summaries might be more likely to contain general terms instead of specific ones, or vice versa.
\vskip 1em
\noindent Stanford NER is also used in a feature group, by counting the number of each type of named entity (person, organization, and location) in a sentence. The motivation for this feature is the intuition that named entities provide important detail about the topic being discussed (such as the "who, what, where" of an article). 
\vskip 1em
\noindent TopicS is used for another feature pertaining to the topic words in set of documents. Sentences contain topic words are likely to be discussing the general topic of an article, as opposed to a specific detail; these sentences should be more highly ranked.
\vskip 1em
\noindent MPQA/GenInq?? is used for a feature based on sentiment analysis. By characterizing the sentiment content of a sentence, it might be possible to gain an idea of its likelihood of occurring in a summary. For example, summaries might contain more neutral language, leaving the stronger sentiments to the more detailed descriptions in a full article. On the other hand, the sentences with strong positive and negative sentiment words may be considered more important to include in a summary.

\subsection{Performance}

\section{Discussion and Analysis}

\begin{thebibliography}{}

\bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
Alfred~V. Aho and Jeffrey~D. Ullman.
\newblock 1972.
\newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
\newblock Prentice-{Hall}, Englewood Cliffs, NJ.

\bibitem[\protect\citename{{American Psychological Association}}1983]{APA:83}
{American Psychological Association}.
\newblock 1983.
\newblock {\em Publications Manual}.
\newblock American Psychological Association, Washington, DC.

\bibitem[\protect\citename{{Association for Computing Machinery}}1983]{ACM:83}
{Association for Computing Machinery}.
\newblock 1983.
\newblock {\em Computing Reviews}, 24(11):503--512.

\bibitem[\protect\citename{Chandra \bgroup et al.\egroup }1981]{Chandra:81}
Ashok~K. Chandra, Dexter~C. Kozen, and Larry~J. Stockmeyer.
\newblock 1981.
\newblock Alternation.
\newblock {\em Journal of the Association for Computing Machinery},
  28(1):114--133.

\bibitem[\protect\citename{Gusfield}1997]{Gusfield:97}
Dan Gusfield.
\newblock 1997.
\newblock {\em Algorithms on Strings, Trees and Sequences}.
\newblock Cambridge University Press, Cambridge, UK.

\end{thebibliography}

\end{document}
